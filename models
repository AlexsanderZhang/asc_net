"""
@ filename: SPP_AEUnet.py
@ 1：replace layer and se-block.
@ 2: add mlp_ratio = 4, aspp_ratio = 0.25, replace mlp-relu to mlp-gelu.
@ 3: mlp_ratio = 3, add softmax behind aspp_pooling, add aspp_pooling potmul proj res.
@ 4: repalce aspp_pooling potmul proj res to aspp_pooling potmul ori_x.
"""
import torch
# import torchvision
from torch import nn
from torch.nn import functional as F
from torchvision import models


class ConvGNRelu(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, drop_ratio: float = 0.5):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.GroupNorm(int(out_ch / 16), out_ch),  # self.bn = nn.BatchNorm2d(out_ch)
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

    def forward(self, x: torch.Tensor):
        x = self.conv(x)
        return x


class Conv1x1GNRelu(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, drop_ratio: float = 0.5):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 1, bias=False),
            nn.GroupNorm(int(out_ch / 16), out_ch),  # self.bn = nn.BatchNorm2d(out_ch)
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

    def forward(self, x: torch.Tensor):
        x = self.conv(x)
        return x


class DoubleConv(nn.Module):
    def __init__(self, in_ch: int, mid_ch: int, out_ch: int, drop_ratio: float = 0.5,):
        super(DoubleConv, self).__init__()
        self.dconv = nn.Sequential(
            nn.Conv2d(in_ch, mid_ch, 3, padding=1, bias=False),
            nn.GroupNorm(int(mid_ch / 16), mid_ch),  # nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio),
            nn.Conv2d(mid_ch, out_ch, 3, padding=1, bias=False),
            nn.GroupNorm(int(out_ch / 16), out_ch),  # nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

    def forward(self, x):
        x = self.dconv(x)
        return x


"""
class DSAConv(nn.Module):
    def __init__(self, in_ch: int, out_ch: int, k_size: int, dilation):
        super(DSAConv, self).__init__()
        self.depthconv = nn.Conv2d()
        self.pointconv = nn.Conv2d()

    def forward(self):
        return
"""


class Interpolate(nn.Module):
    def __init__(
            self,
            size: int = None,
            scale_factor: int = None,
            mode: str = "nearest",
            align_corners: bool = False,
    ):
        super().__init__()
        self.interp = F.interpolate
        self.size = size
        self.mode = mode
        self.scale_factor = scale_factor
        self.align_corners = align_corners

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.interp(
            x,
            size=self.size,
            scale_factor=self.scale_factor,
            mode=self.mode,
            align_corners=self.align_corners,
        )
        return x


class DecoderBlock(nn.Module):
    def __init__(
            self,
            in_channels: int,
            middle_channels: int,
            out_channels: int,
            drop_ratio: float = 0.5,
            is_deconv: bool = False,
    ):
        super().__init__()
        # self.in_channels = in_channels

        if is_deconv:
            self.block = nn.Sequential(
                ConvGNRelu(in_channels, middle_channels),
                nn.ConvTranspose2d(
                    middle_channels, out_channels, kernel_size=4, stride=2, padding=1
                ),
                nn.ReLU(inplace=True),
            )
        else:
            self.block = nn.Sequential(
                Interpolate(scale_factor=2, mode="bilinear"),
                DoubleConv(in_channels, middle_channels, out_channels)
            )

    def forward(self, x: torch.Tensor):
        return self.block(x)


class Mlp(nn.Module):
    def __init__(self, inc: int,
                 ouc: int = 0,
                 mlp_ratio: float = 0.0625,
                 drop_ratio: float = 0.5,
                 is_ouc_diff: bool = False,
                 mlp_bias=True):
        super(Mlp, self).__init__()
        if is_ouc_diff is False:
            self.ouc = inc
        else:
            if ouc == 0:
                self.ouc = int(inc*mlp_ratio)
            else:
                self.ouc = ouc
        self.midc = int(inc*mlp_ratio)
        self.conv1 = nn.Conv2d(inc, self.midc, kernel_size=1, bias=mlp_bias)
        self.act = nn.GELU()
        self.conv2 = nn.Conv2d(self.midc, self.ouc, kernel_size=1, bias=mlp_bias)
        self.drop = nn.Dropout(p=drop_ratio)

    def forward(self, x):
        x = self.drop(self.act(self.conv1(x)))
        x = self.drop(self.conv2(x))
        return x


#   Mlp-ASPP模块
class ASPPConv(nn.Sequential):
    def __init__(self, in_channels, out_channels, dilation):
        super(ASPPConv, self).__init__()
        self.sppconv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),
            nn.GroupNorm(int(out_channels / 16), out_channels),  # nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.sppconv(x)


class ASPPPooling(nn.Sequential):
    def __init__(self, in_channels, out_channels: int = 0, is_interp: bool = False):
        super(ASPPPooling, self).__init__()
        self.pooling = nn.AdaptiveAvgPool2d(1)
        self.mlp = Mlp(in_channels, mlp_ratio=3)
        self.sign = is_interp

    def forward(self, x):
        size = x.shape[-2:]
        x = self.mlp(self.pooling(x))
        # inter_x = F.interpolate(x, size=size, mode='bilinear', align_corners=False)
        return x


class ASPP(nn.Module):
    def __init__(self, in_channels, atrous_rates, aspp_ratio: float = 0.25):
        super(ASPP, self).__init__()
        mid_channels = int(in_channels*aspp_ratio)
        out_channels = in_channels
        # 压缩通道，特征融合
        self.ch_conv = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, 1, bias=False),
            nn.GroupNorm(int(mid_channels/16), mid_channels),  # nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        modules = []
        rate1, rate2, rate3 = tuple(atrous_rates)
        modules.append(ASPPConv(mid_channels, out_channels, rate1))
        modules.append(ASPPConv(mid_channels, out_channels, rate2))
        modules.append(ASPPConv(mid_channels, out_channels, rate3))
        self.convs = nn.ModuleList(modules)

        self.act = nn.Softmax(dim=1)
        self.pool = ASPPPooling(in_channels, out_channels)
        self.project = nn.Sequential(
            nn.Conv2d(int(4 * out_channels), out_channels, 1, bias=False),
            nn.GroupNorm(int(out_channels/16), out_channels),  # nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5))

    def forward(self, x):
        sq_x = self.ch_conv(x)
        res = []
        for conv in self.convs:
            res.append(conv(sq_x))
        sq_x = torch.cat(res, dim=1)
        ca_y = self.act(self.pool(x))
        ca_y = x * ca_y
        sq_x = torch.cat([sq_x, ca_y], dim=1)
        res = self.project(sq_x)
        return res


#   Channel_Attention_Module_block模块
class CAMLayer(nn.Module):
    def __init__(self, in_channel):
        super(CAMLayer, self).__init__()
        self.avg_squeeze = nn.AdaptiveAvgPool2d(1)
        self.max_squeeze = nn.AdaptiveMaxPool2d(1)

        self.mlp = Mlp(in_channel)
        self.act = nn.Sigmoid()

    def forward(self, x):
        y1 = self.mlp(self.avg_squeeze(x))
        y2 = self.mlp(self.max_squeeze(x))
        y = self.act(y1 + y2)
        cam = x * y
        return cam


class SPP_AENet(nn.Module):
    def __init__(self, num_classes: int, num_filters: int = 32, drop_ratio: float = 0.5, is_deconv: bool = False,):
        """
        Args:
            num_filters:
        """
        super().__init__()
        self.channels_r = int(num_filters/16)
        self.pool = nn.MaxPool2d(2, 2)

        self.encoder = models.vgg11(weights=models.VGG11_Weights.DEFAULT).features

        self.conv1 = nn.Sequential(
            self.encoder[0],
            nn.GroupNorm(self.channels_r * 2, num_filters * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

        self.aspp1 = ASPP(num_filters * 2, (6, 12, 18))
        # self.sec1 = CAMLayer(num_filters * 2)

        self.conv2 = nn.Sequential(
            self.encoder[3],
            nn.GroupNorm(self.channels_r * 4, num_filters * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

        self.aspp2 = ASPP(num_filters * 4, (6, 12, 18))
        # self.sec2 = CAMLayer(num_filters * 4)

        self.conv3 = nn.Sequential(
            self.encoder[6],
            nn.GroupNorm(self.channels_r * 8, num_filters * 8),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio),
            self.encoder[8],
            nn.GroupNorm(self.channels_r * 8, num_filters * 8),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

        self.sec3 = CAMLayer(num_filters * 8)
        # self.aspp1 = ASPP(num_filters * 8, (6, 12, 18))
        self.conv4 = nn.Sequential(
            self.encoder[11],
            nn.GroupNorm(self.channels_r * 16, num_filters * 16),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio),
            self.encoder[13],
            nn.GroupNorm(self.channels_r * 16, num_filters * 16),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

        self.sec4 = CAMLayer(num_filters * 16)
        # self.aspp2 = ASPP(num_filters * 16, (6, 12, 18))
        self.conv5 = nn.Sequential(
            self.encoder[16],
            nn.GroupNorm(self.channels_r * 16, num_filters * 16),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio),
            self.encoder[18],
            nn.GroupNorm(self.channels_r * 16, num_filters * 16),
            nn.ReLU(inplace=True),
            nn.Dropout(p=drop_ratio)
        )

        self.sec5 = CAMLayer(num_filters * 16)

        self.dec5 = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8, is_deconv)

        self.sed5 = CAMLayer(num_filters * 8)
        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8, is_deconv)
        self.sed4 = CAMLayer(num_filters * 8)
        self.dec3 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4, is_deconv)
        self.sed3 = CAMLayer(num_filters * 4)
        self.dec2 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2, is_deconv)
        self.sed2 = CAMLayer(num_filters * 2)
        self.dec1 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters, is_deconv)
        self.sed1 = CAMLayer(num_filters * 1)
        self.dec0 = DoubleConv(num_filters * (2 + 1), num_filters, num_filters)

        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        conv1 = self.conv1(x)
        # sec1 = self.sec1(conv1)
        aspp1 = self.aspp1(conv1)
        conv2 = self.conv2(self.pool(conv1))
        # sec2 = self.sec2(conv2)
        aspp2 = self.aspp2(conv2)
        conv3 = self.conv3(self.pool(conv2))
        sec3 = self.sec3(conv3)
        # aspp1 = self.aspp1(conv3)
        conv4 = self.conv4(self.pool(conv3))
        sec4 = self.sec4(conv4)
        # aspp2 = self.aspp2(conv4)
        conv5 = self.conv5(self.pool(conv4))
        sec5 = self.sec5(conv5)

        dec5 = self.dec5(self.pool(conv5))
        # sed5 = self.sed5(dec5)
        # cat5 = torch.cat([conv5, dec5], 1)
        cat5 = torch.cat([dec5, sec5], 1)
        dec4 = self.dec4(cat5)
        # sed4 = self.sed4(dec4)
        # cat4 = torch.cat([dec4, conv4], 1)
        cat4 = torch.cat([dec4, sec4], 1)
        dec3 = self.dec3(cat4)
        # sed3 = self.sed3(dec3)
        # cat3 = torch.cat([dec3, conv3], 1)
        cat3 = torch.cat([dec3, sec3], 1)
        dec2 = self.dec2(cat3)
        # sed2 = self.sed2(dec2)
        # cat2 = torch.cat([dec2, conv2], 1)
        cat2 = torch.cat([dec2, aspp2], 1)
        dec1 = self.dec1(cat2)
        # sed1 = self.sed1(dec1)
        # cat1 = torch.cat([dec1, conv1], 1)
        cat1 = torch.cat([dec1, aspp1], 1)
        dec0 = self.dec0(cat1)
        out = self.final(dec0)
        return out

